{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d2465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "55210832",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "data.drop(['id'], axis=1, inplace=True)\n",
    "tokenzier = SentenceTransformer('msmarco-distilbert-base-v4', device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088ba30",
   "metadata": {},
   "source": [
    "## Uncomment to encode train-inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f306147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = []\n",
    "# batch_size = 128\n",
    "# batch = []\n",
    "# for row in tqdm(data.itertuples()):\n",
    "#     comment_text =row.comment_text\n",
    "#     batch.append(comment_text)\n",
    "#     if len(batch) >= batch_size:\n",
    "#         vectors.append(model.encode(batch))  # Text -> vector encoding happens here\n",
    "#         batch = []\n",
    "\n",
    "# if len(batch) > 0:\n",
    "#     vectors.append(model.encode(batch))\n",
    "#     batch = []\n",
    "\n",
    "# vectors = np.concatenate(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "02585fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.load('sentence-embedding/input_embed.npy', allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9d719beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "Xtrain = vectors[:120000]\n",
    "Xtest = vectors[120000:]\n",
    "y = data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "ytrain = y[:120000]\n",
    "ytest = y[120000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "497ece61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, input, target: pd.DataFrame):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        input_embed = self.input[index]\n",
    "        target_row = self.target.iloc[index]\n",
    "        toxic = target_row['toxic']\n",
    "        severe_toxic = target_row['severe_toxic']\n",
    "        obscene = target_row['obscene']\n",
    "        threat = target_row['threat']\n",
    "        insult = target_row['insult']\n",
    "        identity_hate = target_row['identity_hate']\n",
    "\n",
    "        return dict(\n",
    "            input = input_embed,\n",
    "            toxic = toxic,\n",
    "            severe_toxic = severe_toxic,\n",
    "            obscene = obscene,\n",
    "            threat = threat,\n",
    "            insult = insult,\n",
    "            identity_hate = identity_hate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a6bc6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ToxicDataset(Xtrain, ytrain)\n",
    "test_dataset = ToxicDataset(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a4c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device: str,\n",
    "                 embedding_dim: int = 768,\n",
    "                 num_layers: int = 2,\n",
    "                 num_classes: int = 2) -> 'MainClassifier':\n",
    "        super(MainClassifier, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        torch.manual_seed(42)\n",
    "        self.fc = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.fc.append(nn.Linear(embedding_dim, embedding_dim))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self._classification_layer = nn.Linear(embedding_dim, num_classes).to(device)\n",
    "\n",
    "    def forward(self,\n",
    "             inputs,\n",
    "             training=False):\n",
    "        for i in range(self.num_layers):\n",
    "            if i == self.num_layers - 1:\n",
    "                z = self.fc[i](inputs)\n",
    "                z = self.dropout(z)\n",
    "            else:\n",
    "                z = self.fc[i](inputs)\n",
    "                z = self.dropout(z)\n",
    "                z = self.relu(z)\n",
    "\n",
    "        logits = self._classification_layer(z)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214404a2",
   "metadata": {},
   "source": [
    "## Uncomment to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc09161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toxic_model = MainClassifier(device=device).to(device)\n",
    "# severe_toxic_model = MainClassifier().to(device)\n",
    "# obscene_model = MainClassifier().to(device)\n",
    "# insult_model = MainClassifier().to(device)\n",
    "# identity_hate_model = MainClassifier().to(device)\n",
    "# threat_model = MainClassifier().to(device)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(toxic_model.parameters(), lr=0.001)\n",
    "# model_toxic_dict = train(toxic_model, 'toxic', train_dataset, test_dataset, optimizer=optimizer, num_epochs=5, batch_size=128)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(severe_toxic_model.parameters(), lr=0.001)\n",
    "# model_severe_dict = train(severe_toxic_model, 'severe_toxic', train_dataset, test_dataset, optimizer=optimizer, num_epochs=5, batch_size=128)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(obscene_model.parameters(), lr=0.001)\n",
    "# model_obscene_dict = train(obscene_model, 'obscene', train_dataset, test_dataset, optimizer=optimizer, num_epochs=5, batch_size=128)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(threat_model.parameters(), lr=0.001)\n",
    "# model_threat_dict = train(threat_model, 'threat', train_dataset, test_dataset, optimizer=optimizer, num_epochs=5, batch_size=128)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(insult_model.parameters(), lr=0.001)\n",
    "# model_insult_dict = train(insult_model, 'insult', train_dataset, test_dataset, optimizer=optimizer, num_epochs=5, batch_size=128)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(identity_hate_model.parameters(), lr=0.001)\n",
    "# model_identity_hate_dict = train(identity_hate_model, 'identity_hate', train_dataset, test_dataset, optimizer=optimizer, num_epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c49f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainClassifier(\n",
       "  (fc): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_classification_layer): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_toxic = MainClassifier(device='cpu')\n",
    "state = torch.load('models/toxic_model.pkg', map_location=torch.device('cpu'))\n",
    "model_toxic.load_state_dict(state['model'])\n",
    "model_toxic.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e01837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainClassifier(\n",
       "  (fc): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_classification_layer): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_severe_toxic = MainClassifier(device='cpu')\n",
    "state = torch.load('models/severe_toxic_model.pkg', map_location=torch.device('cpu'))\n",
    "model_severe_toxic.load_state_dict(state['model'])\n",
    "model_severe_toxic.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81db4769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainClassifier(\n",
       "  (fc): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_classification_layer): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_insult = MainClassifier(device='cpu')\n",
    "state = torch.load('models/insult_model.pkg', map_location=torch.device('cpu'))\n",
    "model_insult.load_state_dict(state['model'])\n",
    "model_insult.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba41e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainClassifier(\n",
       "  (fc): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_classification_layer): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_threat = MainClassifier(device='cpu')\n",
    "state = torch.load('models/threat_model.pkg', map_location=torch.device('cpu'))\n",
    "model_threat.load_state_dict(state['model'])\n",
    "model_threat.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da36eb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainClassifier(\n",
       "  (fc): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_classification_layer): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obscene = MainClassifier(device='cpu')\n",
    "state = torch.load('models/obscene_model.pkg', map_location=torch.device('cpu'))\n",
    "model_obscene.load_state_dict(state['model'])\n",
    "model_obscene.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a14f41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainClassifier(\n",
       "  (fc): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_classification_layer): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_identity_hate = MainClassifier(device='cpu')\n",
    "state = torch.load('models/identity_hate_model.pkg', map_location=torch.device('cpu'))\n",
    "model_identity_hate.load_state_dict(state['model'])\n",
    "model_identity_hate.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be8ad8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ae7ef",
   "metadata": {},
   "source": [
    "## Uncomment the following to encode the test-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafaaf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vectors = []\n",
    "# batch_size = 512\n",
    "# batch = []\n",
    "# for row in tqdm(merged_df.itertuples()):\n",
    "#     comment_text =row.comment_text\n",
    "#     batch.append(comment_text)\n",
    "#     if len(batch) >= batch_size:\n",
    "#         test_vectors.append(tokenizer.encode(batch))  # Text -> vector encoding happens here\n",
    "#         batch = []\n",
    "\n",
    "# if len(batch) > 0:\n",
    "#     test_vectors.append(tokenizer.encode(batch))\n",
    "#     batch = []\n",
    "\n",
    "# test_vectors = np.concatenate(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3457d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = torch.from_numpy(np.load('sentence-embedding/test_input_embed.npy', allow_pickle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8057a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_toxic = model_toxic(test_vectors)\n",
    "logits_s_toxic = model_severe_toxic(test_vectors)\n",
    "logits_threat = model_threat(test_vectors)\n",
    "logits_obscene = model_obscene(test_vectors)\n",
    "logits_insult = model_insult(test_vectors)\n",
    "logits_i_hate = model_identity_hate(test_vectors)\n",
    "\n",
    "toxic_predictions = torch.argmax(logits_toxic, dim=1).detach().cpu().numpy()\n",
    "s_toxic_predictions = torch.argmax(logits_s_toxic, dim=1).detach().cpu().numpy()\n",
    "threat_predictions = torch.argmax(logits_threat, dim=1).detach().cpu().numpy()\n",
    "obscene_predictions = torch.argmax(logits_obscene, dim=1).detach().cpu().numpy()\n",
    "insult_predictions = torch.argmax(logits_insult, dim=1).detach().cpu().numpy()\n",
    "hate_predictions = torch.argmax(logits_i_hate, dim=1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cae87eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['toxic'] = toxic_predictions\n",
    "test_df['severe_toxic'] = s_toxic_predictions\n",
    "test_df['threat'] = threat_predictions\n",
    "test_df['obscene'] = obscene_predictions\n",
    "test_df['insult'] = insult_predictions\n",
    "test_df['identity_hate'] = hate_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff0365e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_df.drop(['comment_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40f5e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('prediction_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
